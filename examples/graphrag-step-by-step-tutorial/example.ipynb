{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uCddkwYubxYz",
        "RATtBpbvNpWb"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ngocmai04/GR/blob/release01/examples/graphrag-step-by-step-tutorial/example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('TIDB_HOST')"
      ],
      "metadata": {
        "id": "ihdB6NGpkeDl",
        "outputId": "23e9539e-bad8-489d-ce10-44383fb3cc73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gateway01.ap-southeast-1.prod.aws.tidbcloud.com'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting\n",
        "\n",
        "Before you run this Jupyter Notebook on Colab, please set the secrets:\n",
        "\n",
        "- OPENAI_API_KEY\n",
        "- TIDB_HOST\n",
        "- TIDB_PORT\n",
        "- TIDB_USER\n",
        "- TIDB_PASSWORD\n",
        "- TIDB_DB_NAME\n",
        "\n",
        "For example:\n",
        "\n",
        "![secrets](https://drive.google.com/uc?export=view&id=1meHdytxtx79f2uAFQaOZ8ba3UpwNHkLN)\n",
        "\n",
        "> **Warning:**\n",
        ">\n",
        "> Please aware that this notebook will:\n",
        ">\n",
        "> - Drop some tables and recreate them, please use a new TiDB Serverless cluster.\n",
        "> - Use `Vector` type of TiDB Serverless, please make sure you checked the `Vector Search` feature.\n",
        "> - Use your `OPENAI_API_KEY` to request the OpenAI API via OpenAI client, it will make some bills."
      ],
      "metadata": {
        "id": "xv-ufMosI466"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "uCddkwYubxYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install crawl4ai"
      ],
      "metadata": {
        "id": "Z0FWIruJ_FL4",
        "outputId": "b4289fef-d059-4de3-8b04-0ea1bc10aea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crawl4ai\n",
            "  Downloading crawl4ai-0.5.0.post4-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting aiosqlite~=0.20 (from crawl4ai)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: lxml~=5.3 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (5.3.1)\n",
            "Collecting litellm>=1.53.1 (from crawl4ai)\n",
            "  Downloading litellm-1.63.11-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.26.0)\n",
            "Collecting pillow~=10.4 (from crawl4ai)\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting playwright>=1.49.0 (from crawl4ai)\n",
            "  Downloading playwright-1.51.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv~=1.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.0.1)\n",
            "Requirement already satisfied: requests~=2.26 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4~=4.12 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (4.13.3)\n",
            "Collecting tf-playwright-stealth>=1.1.0 (from crawl4ai)\n",
            "  Downloading tf_playwright_stealth-1.1.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: xxhash~=3.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (3.5.0)\n",
            "Collecting rank-bm25~=0.2 (from crawl4ai)\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting aiofiles>=24.1.0 (from crawl4ai)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting colorama~=0.4 (from crawl4ai)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: snowballstemmer~=2.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.2.0)\n",
            "Collecting pydantic>=2.10 (from crawl4ai)\n",
            "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting pyOpenSSL>=24.3.0 (from crawl4ai)\n",
            "  Downloading pyOpenSSL-25.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting psutil>=6.1.1 (from crawl4ai)\n",
            "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: nltk>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (3.9.1)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (13.9.4)\n",
            "Collecting cssselect>=1.2.0 (from crawl4ai)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.28.1)\n",
            "Collecting fake-useragent>=2.0.3 (from crawl4ai)\n",
            "  Downloading fake_useragent-2.1.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (8.1.8)\n",
            "Requirement already satisfied: pyperclip>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.9.0)\n",
            "Collecting faust-cchardet>=2.1.19 (from crawl4ai)\n",
            "  Downloading faust_cchardet-2.1.19-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: aiohttp>=3.11.11 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (3.11.13)\n",
            "Requirement already satisfied: humanize>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (4.11.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (1.18.3)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.11/dist-packages (from aiosqlite~=0.20->crawl4ai) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4~=4.12->crawl4ai) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->crawl4ai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->crawl4ai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->crawl4ai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->crawl4ai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.2->crawl4ai) (0.14.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (8.6.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (4.23.0)\n",
            "Collecting openai>=1.66.1 (from litellm>=1.53.1->crawl4ai)\n",
            "  Downloading openai-1.66.5-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken>=0.7.0 (from litellm>=1.53.1->crawl4ai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (0.21.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (4.67.1)\n",
            "Collecting pyee<13,>=12 (from playwright>=1.49.0->crawl4ai)\n",
            "  Downloading pyee-12.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright>=1.49.0->crawl4ai) (3.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic>=2.10->crawl4ai)\n",
            "  Using cached pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: cryptography<45,>=41.0.5 in /usr/local/lib/python3.11/dist-packages (from pyOpenSSL>=24.3.0->crawl4ai) (43.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.26->crawl4ai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.26->crawl4ai) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->crawl4ai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->crawl4ai) (2.18.0)\n",
            "Collecting fake-http-header<0.4.0,>=0.3.5 (from tf-playwright-stealth>=1.1.0->crawl4ai)\n",
            "  Downloading fake_http_header-0.3.5-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (1.17.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.23.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->crawl4ai) (0.1.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.1->litellm>=1.53.1->crawl4ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.1->litellm>=1.53.1->crawl4ai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.66.1->litellm>=1.53.1->crawl4ai) (1.3.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm>=1.53.1->crawl4ai) (0.28.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (2.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2023.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (6.0.2)\n",
            "Downloading crawl4ai-0.5.0.post4-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading fake_useragent-2.1.0-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faust_cchardet-2.1.19-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading litellm-1.63.11-py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading playwright-1.51.0-py3-none-manylinux1_x86_64.whl (45.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "Using cached pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "Downloading pyOpenSSL-25.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading tf_playwright_stealth-1.1.2-py3-none-any.whl (33 kB)\n",
            "Downloading fake_http_header-0.3.5-py3-none-any.whl (14 kB)\n",
            "Downloading openai-1.66.5-py3-none-any.whl (571 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.1/571.1 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-12.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faust-cchardet, rank-bm25, pyee, pydantic-core, psutil, pillow, fake-useragent, fake-http-header, cssselect, colorama, aiosqlite, aiofiles, tiktoken, pydantic, playwright, tf-playwright-stealth, pyOpenSSL, openai, litellm, crawl4ai\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.18.4\n",
            "    Uninstalling pydantic_core-2.18.4:\n",
            "      Successfully uninstalled pydantic_core-2.18.4\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.4\n",
            "    Uninstalling pydantic-2.7.4:\n",
            "      Successfully uninstalled pydantic-2.7.4\n",
            "  Attempting uninstall: pyOpenSSL\n",
            "    Found existing installation: pyOpenSSL 24.2.1\n",
            "    Uninstalling pyOpenSSL-24.2.1:\n",
            "      Successfully uninstalled pyOpenSSL-24.2.1\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.27.0\n",
            "    Uninstalling openai-1.27.0:\n",
            "      Successfully uninstalled openai-1.27.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-24.1.0 aiosqlite-0.21.0 colorama-0.4.6 crawl4ai-0.5.0.post4 cssselect-1.3.0 fake-http-header-0.3.5 fake-useragent-2.1.0 faust-cchardet-2.1.19 litellm-1.63.11 openai-1.66.5 pillow-10.4.0 playwright-1.51.0 psutil-7.0.0 pyOpenSSL-25.0.0 pydantic-2.10.6 pydantic-core-2.27.2 pyee-12.1.1 rank-bm25-0.2.2 tf-playwright-stealth-1.1.2 tiktoken-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "OpenSSL",
                  "PIL",
                  "psutil"
                ]
              },
              "id": "d4674c7ad5484501a5d63b65c31d227e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pymysql\n",
        "import dspy\n",
        "import enum\n",
        "import openai\n",
        "\n",
        "from google.colab import userdata\n",
        "from pymysql import Connection\n",
        "from pymysql.cursors import DictCursor\n",
        "from dspy.functional import TypedPredictor\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Mapping, Any, Optional, List\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from pyvis.network import Network\n",
        "from IPython.display import HTML\n",
        "\n",
        "from sqlalchemy import (\n",
        "    Column,\n",
        "    Integer,\n",
        "    String,\n",
        "    Text,\n",
        "    JSON,\n",
        "    ForeignKey,\n",
        "    BLOB,\n",
        "    Enum as SQLEnum,\n",
        "    DateTime,\n",
        "    URL,\n",
        "    create_engine,\n",
        "    or_,\n",
        ")\n",
        "from sqlalchemy.orm import relationship, Session, declarative_base, joinedload\n",
        "from tidb_vector.sqlalchemy import VectorType"
      ],
      "metadata": {
        "id": "J1twp_Mabzuw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites"
      ],
      "metadata": {
        "id": "RATtBpbvNpWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DSPy Part\n",
        "\n",
        "class Entity(BaseModel):\n",
        "    \"\"\"List of entities extracted from the text to form the knowledge graph\"\"\"\n",
        "\n",
        "    name: str = Field(\n",
        "        description=\"Name of the entity, it should be a clear and concise term\"\n",
        "    )\n",
        "    description: str = Field(\n",
        "        description=(\n",
        "            \"Description of the entity, it should be a complete and comprehensive sentence, not few words. \"\n",
        "            \"Sample description of entity 'TiDB in-place upgrade': \"\n",
        "            \"'Upgrade TiDB component binary files to achieve upgrade, generally use rolling upgrade method'\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "class Relationship(BaseModel):\n",
        "    \"\"\"List of relationships extracted from the text to form the knowledge graph\"\"\"\n",
        "\n",
        "    source_entity: str = Field(\n",
        "        description=\"Source entity name of the relationship, it should an existing entity in the Entity list\"\n",
        "    )\n",
        "    target_entity: str = Field(\n",
        "        description=\"Target entity name of the relationship, it should an existing entity in the Entity list\"\n",
        "    )\n",
        "    relationship_desc: str = Field(\n",
        "        description=(\n",
        "            \"Description of the relationship, it should be a complete and comprehensive sentence, not few words. \"\n",
        "            \"Sample relationship description: 'TiDB will release a new LTS version every 6 months.'\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "class KnowledgeGraph(BaseModel):\n",
        "    \"\"\"Graph representation of the knowledge for text.\"\"\"\n",
        "\n",
        "    entities: List[Entity] = Field(\n",
        "        description=\"List of entities in the knowledge graph\"\n",
        "    )\n",
        "    relationships: List[Relationship] = Field(\n",
        "        description=\"List of relationships in the knowledge graph\"\n",
        "    )\n",
        "\n",
        "class ExtractGraphTriplet(dspy.Signature):\n",
        "    \"\"\"Carefully analyze the provided text from database documentation and community blogs to thoroughly identify all entities related to database technologies, including both general concepts and specific details.\n",
        "\n",
        "    Follow these Step-by-Step Analysis:\n",
        "\n",
        "    1. Extract Meaningful Entities:\n",
        "      - Identify all significant nouns, proper nouns, and technical terminologies that represent database-related concepts, objects, components, features, issues, key steps, execute order, user case, locations, versions, or any substantial entities.\n",
        "      - Ensure that you capture entities across different levels of detail, from high-level overviews to specific technical specifications, to create a comprehensive representation of the subject matter.\n",
        "      - Choose names for entities that are specific enough to indicate their meaning without additional context, avoiding overly generic terms.\n",
        "      - Consolidate similar entities to avoid redundancy, ensuring each represents a distinct concept at appropriate granularity levels.\n",
        "\n",
        "    2. Establish Relationships:\n",
        "      - Carefully examine the text to identify all relationships between clearly-related entities, ensuring each relationship is correctly captured with accurate details about the interactions.\n",
        "      - Analyze the context and interactions between the identified entities to determine how they are interconnected, focusing on actions, associations, dependencies, or similarities.\n",
        "      - Clearly define the relationships, ensuring accurate directionality that reflects the logical or functional dependencies among entities. \\\n",
        "         This means identifying which entity is the source, which is the target, and what the nature of their relationship is (e.g., $source_entity depends on $target_entity for $relationship).\n",
        "\n",
        "    Some key points to consider:\n",
        "      - Please endeavor to extract all meaningful entities and relationships from the text, avoid subsequent additional gleanings.\n",
        "\n",
        "    Objective: Produce a detailed and comprehensive knowledge graph that captures the full spectrum of entities mentioned in the text, along with their interrelations, reflecting both broad concepts and intricate details specific to the database domain.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    text = dspy.InputField(\n",
        "        desc=\"a paragraph of text to extract entities and relationships to form a knowledge graph\"\n",
        "    )\n",
        "    knowledge: KnowledgeGraph = dspy.OutputField(\n",
        "        desc=\"Graph representation of the knowledge extracted from the text.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class Extractor(dspy.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.prog_graph = TypedPredictor(ExtractGraphTriplet)\n",
        "\n",
        "    def forward(self, text):\n",
        "        return self.prog_graph(\n",
        "            text=text,\n",
        "            config={\n",
        "                \"response_format\": {\"type\": \"json_object\"},\n",
        "            },\n",
        "        )\n",
        "\n",
        "def jupyter_interactive_graph(kg: KnowledgeGraph) -> str:\n",
        "    net = Network(notebook=True, cdn_resources='remote')\n",
        "\n",
        "    node_map = {}\n",
        "    for index in range(len(kg.entities)):\n",
        "        node_map[kg.entities[index].name] = index\n",
        "        net.add_node(\n",
        "            index,\n",
        "            label=kg.entities[index].name,\n",
        "            title=kg.entities[index].description\n",
        "        )\n",
        "\n",
        "    for index in range(len(kg.relationships)):\n",
        "        relation = kg.relationships[index]\n",
        "        src_index = node_map[relation.source_entity]\n",
        "        target_index = node_map[relation.target_entity]\n",
        "        net.add_edge(src_index, target_index)\n",
        "\n",
        "    filename = \"kg.html\"\n",
        "    net.save_graph(filename)\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "# OpenAI Part\n",
        "\n",
        "def get_query_embedding(query: str):\n",
        "    open_ai_client = openai.OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "    response = open_ai_client.embeddings.create(input=[query], model=\"text-embedding-3-small\")\n",
        "    return response.data[0].embedding\n",
        "\n",
        "\n",
        "def generate_result(query: str, entities, relationships):\n",
        "    open_ai_client = openai.OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "    entities_prompt = '\\n'.join(map(lambda e: f'(Name: \"{e.name}\", Description: \"{e.description}\")', entities))\n",
        "    relationships_prompt = '\\n'.join(map(lambda r: f'\"{r.relationship_desc}\"', relationships))\n",
        "\n",
        "    response = open_ai_client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Please carefully think the user's \" +\n",
        "             \"question and ONLY use the content below to generate answer:\\n\" +\n",
        "             f\"Entities: {entities_prompt}, Relationships: {relationships_prompt}\"},\n",
        "            {\"role\": \"user\", \"content\": query}\n",
        "        ])\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# TiDB Serverless Database Part\n",
        "\n",
        "def get_db_url():\n",
        "    database_name = userdata.get(\"TIDB_DB_NAME\")\n",
        "    # Remove any carriage return or newline characters from the database name\n",
        "    database_name = database_name.strip() # Add this line to strip whitespace characters\n",
        "\n",
        "    return URL(\n",
        "        drivername=\"mysql+pymysql\",\n",
        "        username=userdata.get(\"TIDB_USER\"),\n",
        "        password=userdata.get(\"TIDB_PASSWORD\"),\n",
        "        host=userdata.get('TIDB_HOST'),\n",
        "        port=int(userdata.get(\"TIDB_PORT\")),\n",
        "        database=database_name,  # Use the stripped database name\n",
        "        query={\"ssl_verify_cert\": True, \"ssl_verify_identity\": True},\n",
        "    )\n",
        "engine = create_engine(get_db_url(), pool_recycle=300)\n",
        "Base = declarative_base()\n",
        "\n",
        "class DatabaseEntity(Base):\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    name = Column(String(512))\n",
        "    description = Column(Text)\n",
        "    description_vec = Column(VectorType())\n",
        "\n",
        "    __tablename__ = \"entities\"\n",
        "\n",
        "\n",
        "class DatabaseRelationship(Base):\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    source_entity_id = Column(Integer, ForeignKey(\"entities.id\"))\n",
        "    target_entity_id = Column(Integer, ForeignKey(\"entities.id\"))\n",
        "    relationship_desc = Column(Text)\n",
        "\n",
        "    source_entity = relationship(\"DatabaseEntity\", foreign_keys=[source_entity_id])\n",
        "    target_entity = relationship(\"DatabaseEntity\", foreign_keys=[target_entity_id])\n",
        "\n",
        "    __tablename__ = \"relationships\"\n",
        "\n",
        "def save_knowledge_graph(kg: KnowledgeGraph):\n",
        "    data_entities = list(map(lambda e: DatabaseEntity(\n",
        "        name = e.name,\n",
        "        description = e.description,\n",
        "        description_vec = get_query_embedding(e.description)\n",
        "    ), kg.entities))\n",
        "\n",
        "    with Session(engine) as session:\n",
        "        session.add_all(data_entities)\n",
        "        # get increment ids\n",
        "        session.flush()\n",
        "\n",
        "        entity_id_map = dict(map(lambda e: (e.name, e.id), data_entities))\n",
        "        data_relationships = list(map(lambda r: DatabaseRelationship(\n",
        "            source_entity_id = entity_id_map[r.source_entity],\n",
        "            target_entity_id = entity_id_map[r.target_entity],\n",
        "            relationship_desc = r.relationship_desc\n",
        "        ), kg.relationships))\n",
        "\n",
        "        session.add_all(data_relationships)\n",
        "        session.commit()\n",
        "\n",
        "def retrieve_entities_relationships(question_embedding) -> (List[DatabaseEntity], List[DatabaseRelationship]) :\n",
        "    with Session(engine) as session:\n",
        "        entity = session.query(DatabaseEntity) \\\n",
        "            .order_by(DatabaseEntity.description_vec.cosine_distance(question_embedding)) \\\n",
        "            .limit(1).first()\n",
        "        entities = {entity.id: entity}\n",
        "\n",
        "        relationships = session.query(DatabaseRelationship).options(\n",
        "            joinedload(DatabaseRelationship.source_entity),\n",
        "            joinedload(DatabaseRelationship.target_entity),\n",
        "        ).filter(\n",
        "            or_(\n",
        "                DatabaseRelationship.source_entity == entity,\n",
        "                DatabaseRelationship.target_entity == entity\n",
        "            )\n",
        "        )\n",
        "\n",
        "        for r in relationships:\n",
        "            entities.update({\n",
        "                r.source_entity.id: r.source_entity,\n",
        "                r.target_entity.id: r.target_entity,\n",
        "            })\n",
        "\n",
        "        return entities.values(), relationships\n",
        "\n",
        "# Initial\n",
        "\n",
        "extractor = Extractor()\n",
        "Base.metadata.drop_all(engine)\n",
        "Base.metadata.create_all(engine)"
      ],
      "metadata": {
        "id": "UPYR-X_e78N-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Core Code"
      ],
      "metadata": {
        "id": "Mal3vFr5ZhKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1. Indexing\n",
        "\n",
        "Indexing in terms of RAG is the process of organizing a vast amount of text data in a way that allows the RAG system to quickly find the most relevant pieces of information for a given query. [\\[1\\]](https://medium.com/@j13mehul/rag-part-4-indexing-1985f4000f72#)"
      ],
      "metadata": {
        "id": "7hPnNXv0OMqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set OpenAI and DSPy"
      ],
      "metadata": {
        "id": "GhlzJOqHZmGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "open_ai_client = dspy.OpenAI(model=\"gpt-4o\", api_key=userdata.get('OPENAI_API_KEY'), max_tokens=4096)\n",
        "dspy.settings.configure(lm=open_ai_client)"
      ],
      "metadata": {
        "id": "GalPqH4bZn5k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Raw Wikipedia Page"
      ],
      "metadata": {
        "id": "fxMzegLUZuNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wiki = WikipediaLoader(query=\"Elon Musk\").load()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XxASIQfcDxk_",
        "outputId": "c6dddc40-df7d-40d7-c419-1a1e4a7c487c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.11/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Raw Wikipedia Page to Knowledge Graph"
      ],
      "metadata": {
        "id": "Mv8fv4VPZ6mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = extractor(text = wiki[0].page_content)"
      ],
      "metadata": {
        "id": "V6KywhEJMe_E",
        "outputId": "7a14c589-9f47-4965-9564-e9a8545aff46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 1.0 seconds after 1 tries calling function <function GPT3.request at 0x79305d211940> with kwargs {'response_format': {'type': 'json_object'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 1.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 1.5 seconds after 2 tries calling function <function GPT3.request at 0x79305d211940> with kwargs {'response_format': {'type': 'json_object'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 0.4 seconds after 3 tries calling function <function GPT3.request at 0x79305d211940> with kwargs {'response_format': {'type': 'json_object'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 4.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 4.5 seconds after 4 tries calling function <function GPT3.request at 0x79305d211940> with kwargs {'response_format': {'type': 'json_object'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 9.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 9.6 seconds after 5 tries calling function <function GPT3.request at 0x79305d211940> with kwargs {'response_format': {'type': 'json_object'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 8.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 8.5 seconds after 6 tries calling function <function GPT3.request at 0x79305d211940> with kwargs {'response_format': {'type': 'json_object'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 40.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 40.6 seconds after 7 tries calling function <function GPT3.request at 0x79305d211940> with kwargs {'response_format': {'type': 'json_object'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 96.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 96.4 seconds after 8 tries calling function <function GPT3.request at 0x79305d211940> with kwargs {'response_format': {'type': 'json_object'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 101.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 101.2 seconds after 9 tries calling function <function GPT3.request at 0x79305d211940> with kwargs {'response_format': {'type': 'json_object'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 316.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 316.1 seconds after 10 tries calling function <function GPT3.request at 0x79305d211940> with kwargs {'response_format': {'type': 'json_object'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:backoff:Backing off request(...) for 272.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backing off 272.3 seconds after 11 tries calling function <function GPT3.request at 0x79305d211940> with kwargs {'response_format': {'type': 'json_object'}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's Show the Graph"
      ],
      "metadata": {
        "id": "5Q1aZdrdaIX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HTML(filename=jupyter_interactive_graph(pred.knowledge))"
      ],
      "metadata": {
        "id": "8cM2hau4S3kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Graph to TiDB Serverless"
      ],
      "metadata": {
        "id": "j139JoygEgc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_knowledge_graph(pred.knowledge)"
      ],
      "metadata": {
        "id": "YXvTxGbIEnPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2. Retrieve\n",
        "\n",
        "After indexing, we can retrieve data from the graph."
      ],
      "metadata": {
        "id": "Y0zLR_ZuqsXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ask Question"
      ],
      "metadata": {
        "id": "aCAeZfZd992I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who is Elon Musk?\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "DWw2lyLmEf-P",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find Entites and Relationships\n",
        "\n",
        "In this case, we will get the nearest entities, by using embedding vector which the feature offered by TiDB Serverless. Then, get the nearest neighbors of this node, and the relationships between them.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1lwslklL5eaX_YMY_i4TDXIbRsJiqhJEV\" width=\"400\">\n",
        "\n"
      ],
      "metadata": {
        "id": "kcvp8wi9En-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_embedding = get_query_embedding(question)\n",
        "entities, relationships = retrieve_entities_relationships(question_embedding)"
      ],
      "metadata": {
        "id": "nbtaoDbtxEUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3. Generate Answer\n",
        "\n",
        "Once we got the entities and relationships, we can generate the answer by laveraging the LLM. We can limit it by  "
      ],
      "metadata": {
        "id": "RP6w4W2p-zz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = generate_result(question, entities, relationships)\n",
        "result"
      ],
      "metadata": {
        "id": "yHyc9qFb-zQJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}